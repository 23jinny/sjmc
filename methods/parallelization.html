

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>8. Parallelization &mdash; OpenMC Documentation</title>
    
    <link rel="stylesheet" href="../_static/haiku.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/print.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.5.3',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/theme_extras.js"></script>
    <link rel="top" title="OpenMC Documentation" href="../index.html" />
    <link rel="up" title="Theory and Methodology" href="index.html" />
    <link rel="next" title="User’s Guide" href="../usersguide/index.html" />
    <link rel="prev" title="7. Eigenvalue Calculations" href="eigenvalue.html" /> 
  </head>
  <body>
      <div class="header">
        <a href="../index.html">
          <img class="logo" src="../_static/openmc.png" alt="Logo"/>
        </a>
      </div>
      <div class="topnav">
      
        <p>
        «&#160;&#160;<a href="eigenvalue.html">7. Eigenvalue Calculations</a>
        &#160;&#160;::&#160;&#160;
        <a class="uplink" href="../index.html">Contents</a>
        &#160;&#160;::&#160;&#160;
        <a href="../usersguide/index.html">User&#8217;s Guide</a>&#160;&#160;»
        </p>

      </div>
      <div class="content">
        
        
  <div class="section" id="parallelization">
<span id="methods-parallel"></span><h1>8. Parallelization<a class="headerlink" href="#parallelization" title="Permalink to this headline">¶</a></h1>
<p>Due to the computationally-intensive nature of Monte Carlo methods, there has
been an ever-present interest in parallelizing such simulations. Even in the
<a class="reference external" href="http://www.jstor.org/stable/2280232">first paper</a> on the Monte Carlo method, John Metropolis and Stanislaw Ulam
recognized that solving the Boltzmann equation with the Monte Carlo method could
be done in parallel very easily whereas the deterministic counterparts for
solving the Boltzmann equation did not offer such a natural means of
parallelism. With the introduction of <a class="reference external" href="http://en.wikipedia.org/wiki/Vector_processor">vector computers</a> in the early 1970s,
general-purpose parallel computing became a reality. In 1972, Troubetzkoy et
al. designed a Monte Carlo code to be run on the first vector computer, the
ILLIAC-IV <a class="reference internal" href="#troubetzkoy">[Troubetzkoy]</a>. The general principles from that work were later
refined and extended greatly through the <a class="reference external" href="http://hdl.handle.net/2027.42/24996">work of Forrest Brown</a> in the
1980s. However, as Brown&#8217;s work shows, the <a class="reference external" href="http://en.wikipedia.org/wiki/SIMD">single-instruction multiple-data</a>
(SIMD) parallel model inherent to vector processing does not lend itself to the
parallelism on particles in Monte Carlo simulations. Troubetzkoy et
al. recognized this, remarking that &#8220;the order and the nature of these physical
events have little, if any, correlation from history to history,&#8221; and thus
following independent particle histories simultaneously using a SIMD model is
difficult.</p>
<p>The difficulties with vector processing of Monte Carlo codes led to the adoption
of the <a class="reference external" href="http://en.wikipedia.org/wiki/SPMD">single program multiple data</a> (SPMD) technique for parallelization. In
this model, each different process tracks a particle independently of other
processes, and between fission source generations the processes communicate data
through a <a class="reference external" href="http://en.wikipedia.org/wiki/Message_Passing_Interface">message-passing interface</a>. This means of parallelism was enabled by
the introduction of message-passing standards in the late 1980s and early 1990s
such as <a class="reference external" href="http://www.csm.ornl.gov/pvm/pvm_home.html">PVM</a> and <a class="reference external" href="http://www.mcs.anl.gov/research/projects/mpi/">MPI</a>. The SPMD model proved much easier to use in practice and
took advantage of the inherent parallelism on particles rather than
instruction-level parallelism. As a result, it has since become ubiquitous for
Monte Carlo simulations of transport phenomena.</p>
<p>Thanks to the particle-level parallelism using SPMD techniques, extremely high
parallel efficiencies could be achieved in Monte Carlo codes. Until the last
decade, even the most demanding problems did not require transmitting large
amounts of data between processors, and thus the total amount of time spent on
communication was not significant compared to the amount of time spent on
computation. However, today&#8217;s computing power has created a demand for
increasingly large and complex problems, requiring a greater number of particles
to obtain decent statistics (and convergence in the case of criticality
calculations). This results in a correspondingly higher amount of communication,
potentially degrading the parallel efficiency. Thus, while Monte Carlo
simulations may seem <a class="reference external" href="http://en.wikipedia.org/wiki/Embarrassingly_parallel">embarrassingly parallel</a>, obtaining good parallel scaling
with large numbers of processors can be quite difficult to achieve in practice.</p>
<div class="section" id="fission-bank-algorithms">
<span id="id2"></span><h2>8.1. Fission Bank Algorithms<a class="headerlink" href="#fission-bank-algorithms" title="Permalink to this headline">¶</a></h2>
<div class="section" id="master-slave-algorithm">
<h3>8.1.1. Master-Slave Algorithm<a class="headerlink" href="#master-slave-algorithm" title="Permalink to this headline">¶</a></h3>
<p>Monte Carlo particle transport codes commonly implement a SPMD model by having
one master process that controls the scheduling of work and the remaining
processes wait to receive work from the master, process the work, and then send
their results to the master at the end of the simulation (or a source iteration
in the case of an eigenvalue calculation). This idea is illustrated in
<a class="reference internal" href="#figure-master-slave"><em>Figure 1</em></a>.</p>
<div class="align-center figure align-center" id="figure-master-slave">
<img alt="../_images/master-slave.png" src="../_images/master-slave.png" />
<p class="caption"><strong>Figure 1</strong>: Communication pattern in master-slave algorithm.</p>
</div>
<p>Eigenvalue calculations are slightly more difficult to parallelize than fixed
source calculations since it is necessary to converge on the fission source
distribution and eigenvalue before tallying. In the
<a class="reference internal" href="eigenvalue.html#method-successive-generations"><em>Method of Successive Generations</em></a>, to ensure that the results are
reproducible, one must guarantee that the process by which fission sites are
randomly sampled does not depend on the number of processors. What is typically
done is the following:</p>
<blockquote>
<div><ol class="arabic simple">
<li>Each compute node <a class="reference external" href="http://www.mcs.anl.gov/research/projects/mpi/www/www3/MPI_Send.html">sends</a> its fission bank sites to a master process;</li>
</ol>
<p>2. The master process sorts or orders the fission sites based on a unique
identifier;</p>
<p>3. The master process samples <img class="math" src="../_images/math/fc97ef67268cd4e91bacdf12b8901d7036c9a056.png" alt="N"/> fission sites from the ordered array
of <img class="math" src="../_images/math/5d1e4485dc90c450e8c76826516c1b2ccb8fce16.png" alt="M"/> sites; and</p>
<p>4. The master process <a class="reference external" href="http://www.mcs.anl.gov/research/projects/mpi/www/www3/MPI_Bcast.html">broadcasts</a> all the fission sites to the compute
nodes.</p>
</div></blockquote>
<p>The first and last steps of this process are the major sources of communication
overhead between cycles. Since the master process must receive <img class="math" src="../_images/math/5d1e4485dc90c450e8c76826516c1b2ccb8fce16.png" alt="M"/> fission
sites from the compute nodes, the first step is necessarily serial. This step
can be completed in <img class="math" src="../_images/math/f40785dd640fc0499b257dfb890c1447c3deb48b.png" alt="O(M)"/> time. The broadcast step can benefit from
parallelization through a tree-based algorithm. Despite this, the communication
overhead is still considerable.</p>
<p>To see why this is the case, it is instructive to look at a hypothetical
example. Suppose that a calculation is run with <img class="math" src="../_images/math/3bc0d24b5b9506c56f1ffb5e97c49ebed9b50491.png" alt="N = 10,000,000"/> neutrons
across 64 compute nodes. On average, <img class="math" src="../_images/math/6dcdb8fdff0f050c7f4a0690fc4b1be5f71ab01f.png" alt="M = 10,000,000"/> fission sites will
be produced. If the data for each fission site consists of a spatial location
(three 8 byte real numbers) and a unique identifier (one 4 byte integer), the
memory required per site is 28 bytes. To broadcast 10,000,000 source sites to 64
nodes will thus require transferring 17.92 GB of data.  Since each compute node
does not need to keep every source site in memory, one could modify the
algorithm from a broadcast to a <a class="reference external" href="http://www.mcs.anl.gov/research/projects/mpi/www/www3/MPI_Scatter.html">scatter</a>. However, for practical reasons
(e.g. work self-scheduling), this is normally not done in production Monte Carlo
codes.</p>
</div>
<div class="section" id="nearest-neighbors-algorithm">
<span id="id3"></span><h3>8.1.2. Nearest Neighbors Algorithm<a class="headerlink" href="#nearest-neighbors-algorithm" title="Permalink to this headline">¶</a></h3>
<p>To reduce the amount of communication required in a fission bank synchronization
algorithm, it is desirable to move away from the typical master-slave algorithm
to an algorithm whereby the compute nodes communicate with one another only as
needed. This concept is illustrated in <a class="reference internal" href="#figure-nearest-neighbor"><em>Figure 2</em></a>.</p>
<div class="align-center figure align-center" id="figure-nearest-neighbor">
<img alt="../_images/nearest-neighbor.png" src="../_images/nearest-neighbor.png" />
<p class="caption"><strong>Figure 2</strong>: Communication pattern in nearest neighbor algorithm.</p>
</div>
<p>Since the source sites for each cycle are sampled from the fission sites banked
from the previous cycle, it is a common occurrence for a fission site to be
banked on one compute node and sent back to the master only to get sent back to
the same compute node as a source site. As a result, much of the communication
inherent in the algorithm described previously is entirely unnecessary. By
keeping the fission sites local, having each compute node sample fission sites,
and sending sites between nodes only as needed, one can cut down on most of the
communication. One algorithm to achieve this is as follows:</p>
<blockquote>
<div><p>1. An exclusive scan is performed on the number of sites banked, and the
total number of fission bank sites is broadcasted to all compute nodes. By
picturing the fission bank as one large array distributed across multiple
nodes, one can see that this step enables each compute node to determine the
starting index of fission bank sites in this array. Let us call the starting
and ending indices on the <img class="math" src="../_images/math/34857b3ba74ce5cd8607f3ebd23e9015908ada71.png" alt="i"/>-th node <img class="math" src="../_images/math/412787c048e28774dc63fc27db42dc52ca858de7.png" alt="a_i"/> and <img class="math" src="../_images/math/94d9565abaadf04609a2e9941aa2d20b0a299b8a.png" alt="b_i"/>,
respectively;</p>
<p>2. Each compute node samples sites at random from the fission bank using the
same starting seed. A separate array on each compute node is created that
consists of sites that were sampled local to that node, i.e. if the index of
the sampled site is between <img class="math" src="../_images/math/412787c048e28774dc63fc27db42dc52ca858de7.png" alt="a_i"/> and <img class="math" src="../_images/math/94d9565abaadf04609a2e9941aa2d20b0a299b8a.png" alt="b_i"/>, it is set aside;</p>
<p>3. If any node sampled more than <img class="math" src="../_images/math/c395cd3824fd2b81862d12d44397aa041de3156a.png" alt="N/p"/> fission sites where <img class="math" src="../_images/math/36f73fc1312ee0349b3f3a0f3bd9eb5504339011.png" alt="p"/>
is the number of compute nodes, the extra sites are put in a separate array
and sent to all other compute nodes. This can be done efficiently using the
<a class="reference external" href="http://www.mcs.anl.gov/research/projects/mpi/www/www3/MPI_Allgather.html">allgather</a> collective operation;</p>
<p>4. The extra sites are divided among those compute nodes that sampled fewer
than <img class="math" src="../_images/math/c395cd3824fd2b81862d12d44397aa041de3156a.png" alt="N/p"/> fission sites.</p>
</div></blockquote>
<p>However, even this algorithm exhibits more communication than necessary since
the allgather will send fission bank sites to nodes that don&#8217;t necessarily
need any extra sites.</p>
<p>One alternative is to replace the allgather with a series of sends. If
<img class="math" src="../_images/math/412787c048e28774dc63fc27db42dc52ca858de7.png" alt="a_i"/> is less than <img class="math" src="../_images/math/9e7df94ed2e19437782bf0c98ca62c08891b575c.png" alt="iN/p"/>, then send <img class="math" src="../_images/math/2d5f4ceafc5abd4d181d7fab657ad2a00b52514d.png" alt="iN/p - a_i"/> sites to the
left adjacent node. Similarly, if <img class="math" src="../_images/math/412787c048e28774dc63fc27db42dc52ca858de7.png" alt="a_i"/> is greater than <img class="math" src="../_images/math/9e7df94ed2e19437782bf0c98ca62c08891b575c.png" alt="iN/p"/>, then
receive <img class="math" src="../_images/math/031bda9c1155058828804c1eb69eac1050f7d292.png" alt="a_i - iN/p"/> from the left adjacent node. This idea is applied to
the fission bank sites at the end of each node&#8217;s array as well. If <img class="math" src="../_images/math/94d9565abaadf04609a2e9941aa2d20b0a299b8a.png" alt="b_i"/>
is less than <img class="math" src="../_images/math/1fba80badc9b7495c1a36311addc9db5d182a1ba.png" alt="(i+1)N/p"/>, then receive <img class="math" src="../_images/math/2b4aa5b6ad0ddfee677ea7fd322663caf0bc6927.png" alt="(i+1)N/p - b_i"/> sites from
the right adjacent node. If <img class="math" src="../_images/math/94d9565abaadf04609a2e9941aa2d20b0a299b8a.png" alt="b_i"/> is greater than <img class="math" src="../_images/math/1fba80badc9b7495c1a36311addc9db5d182a1ba.png" alt="(i+1)N/p"/>, then
send <img class="math" src="../_images/math/e352425fa8c8678769e567e6d6645a91e117f447.png" alt="b_i - (i+1)N/p"/> sites to the right adjacent node. Thus, each compute
node sends/receives only two messages under normal circumstances.</p>
<p>The following example illustrates how this algorithm works. Let us suppose we
are simulating <img class="math" src="../_images/math/82b5e178d68dd7eb9f2394965b656f306a12510b.png" alt="N = 1000"/> neutrons across four compute nodes. For this
example, it is instructive to look at the state of the fission bank and source
bank at several points in the algorithm:</p>
<blockquote>
<div><ol class="arabic simple">
<li>The beginning of a cycle where each node has <img class="math" src="../_images/math/c395cd3824fd2b81862d12d44397aa041de3156a.png" alt="N/p"/> source sites;</li>
<li>The end of a cycle where each node has accumulated fission sites;</li>
</ol>
<p>3. After sampling, where each node has some amount of source sites usually
not equal to <img class="math" src="../_images/math/c395cd3824fd2b81862d12d44397aa041de3156a.png" alt="N/p"/>;</p>
<p>4. After redistribution, each node again has <img class="math" src="../_images/math/c395cd3824fd2b81862d12d44397aa041de3156a.png" alt="N/p"/> source sites for
the next cycle;</p>
</div></blockquote>
<p>At the end of each cycle, each compute node needs 250 fission bank sites to
continue on the next cycle. Let us suppose that <img class="math" src="../_images/math/d4e8cf09c5ced63a38ed87c9a03dd9aacf7e490a.png" alt="p_0"/> produces 270 fission
banks sites, <img class="math" src="../_images/math/e01486dcc7cb25518b89692706b17389dc67c92b.png" alt="p_1"/> produces 230, <img class="math" src="../_images/math/a2e8bc2235f5b0094bc37f2ef74f7dc647f0a406.png" alt="p_2"/> produces 290, and <img class="math" src="../_images/math/d05d6a0d3fdb2123690c1b35c922d1e082e810f1.png" alt="p_3"/>
produces 250. After each node samples from its fission bank sites, let&#8217;s assume
that <img class="math" src="../_images/math/d4e8cf09c5ced63a38ed87c9a03dd9aacf7e490a.png" alt="p_0"/> has 260 source sites, <img class="math" src="../_images/math/e01486dcc7cb25518b89692706b17389dc67c92b.png" alt="p_1"/> has 215, <img class="math" src="../_images/math/a2e8bc2235f5b0094bc37f2ef74f7dc647f0a406.png" alt="p_2"/> has 280,
and <img class="math" src="../_images/math/d05d6a0d3fdb2123690c1b35c922d1e082e810f1.png" alt="p_3"/> has 245. Note that the total number of sampled sites is 1000 as
needed. For each node to have the same number of source sites, <img class="math" src="../_images/math/d4e8cf09c5ced63a38ed87c9a03dd9aacf7e490a.png" alt="p_0"/> needs
to send its right-most 10 sites to <img class="math" src="../_images/math/e01486dcc7cb25518b89692706b17389dc67c92b.png" alt="p_1"/>, and <img class="math" src="../_images/math/a2e8bc2235f5b0094bc37f2ef74f7dc647f0a406.png" alt="p_2"/> needs to send
its left-most 25 sites to <img class="math" src="../_images/math/e01486dcc7cb25518b89692706b17389dc67c92b.png" alt="p_1"/> and its right-most 5 sites to
<img class="math" src="../_images/math/d05d6a0d3fdb2123690c1b35c922d1e082e810f1.png" alt="p_3"/>. A schematic of this example is shown in <a class="reference internal" href="#figure-neighbor-example"><em>Figure 3</em></a>. The data local to each node is given a different
hatching, and the cross-hatched regions represent source sites that are
communicated between adjacent nodes.</p>
<div class="align-center figure align-center" id="figure-neighbor-example">
<img alt="../_images/nearest-neighbor-example.png" src="../_images/nearest-neighbor-example.png" />
<p class="caption"><strong>Figure 3</strong>: Example of nearest neighbor algorithm.</p>
</div>
</div>
<div class="section" id="cost-of-master-slave-algorithm">
<span id="master-slave-cost"></span><h3>8.1.3. Cost of Master-Slave Algorithm<a class="headerlink" href="#cost-of-master-slave-algorithm" title="Permalink to this headline">¶</a></h3>
<p>While the prior considerations may make it readily apparent that the novel
algorithm should outperform the traditional algorithm, it is instructive to look
at the total communication cost of the novel algorithm relative to the
traditional algorithm. This is especially so because the novel algorithm does
not have a constant communication cost due to stochastic fluctuations. Let us
begin by looking at the cost of communication in the traditional algorithm</p>
<p>As discussed earlier, the traditional algorithm is composed of a series of sends
and typically a broadcast. To estimate the communication cost of the algorithm,
we can apply a simple model that captures the essential features. In this model,
we assume that the time that it takes to send a message between two nodes is
given by <img class="math" src="../_images/math/36f98a9555d87144fc3c2fd166cec3f261dc7364.png" alt="\alpha + (sN)\beta"/>, where <img class="math" src="../_images/math/10f32377ac67d94f764f12a15ea987e88c85d3e1.png" alt="\alpha"/> is the time it takes
to initiate the communication (commonly called the <a class="reference external" href="http://en.wikipedia.org/wiki/Latency_(engineering)#Packet-switched_networks">latency</a>), <img class="math" src="../_images/math/fdb63b9e51abe6bbb16acfb5d7b773ddbb5bf4a8.png" alt="\beta"/> is
the transfer time per unit of data (commonly called the <a class="reference external" href="http://en.wikipedia.org/wiki/Bandwidth_(computing)">bandwidth</a>), <img class="math" src="../_images/math/fc97ef67268cd4e91bacdf12b8901d7036c9a056.png" alt="N"/>
is the number of fission sites, and <img class="math" src="../_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/> is the size in bytes of each
fission site.</p>
<p>The first step of the traditional algorithm is to send <img class="math" src="../_images/math/36f73fc1312ee0349b3f3a0f3bd9eb5504339011.png" alt="p"/> messages to the
master node, each of size <img class="math" src="../_images/math/fedbcdb9cda6169277d9b733d69494b45ceae1fc.png" alt="sN/p"/>. Thus, the total time to send these
messages is</p>
<div class="math" id="equation-t-send">
<p><span class="eqno">(1)</span><img src="../_images/math/9d7c8ec12f7a641022948e5850e3bab46f27356c.png" alt="t_{\text{send}} = p\alpha + sN\beta."/></p>
</div><p>Generally, the best parallel performance is achieved in a weak scaling scheme
where the total number of histories is proportional to the number of
processors. However, we see that when <img class="math" src="../_images/math/fc97ef67268cd4e91bacdf12b8901d7036c9a056.png" alt="N"/> is proportional to <img class="math" src="../_images/math/36f73fc1312ee0349b3f3a0f3bd9eb5504339011.png" alt="p"/>,
the time to send these messages increases proportionally with <img class="math" src="../_images/math/36f73fc1312ee0349b3f3a0f3bd9eb5504339011.png" alt="p"/>.</p>
<p>Estimating the time of the broadcast is complicated by the fact that different
MPI implementations may use different algorithms to perform collective
communications. Worse yet, a single implementation may use a different algorithm
depending on how many nodes are communicating and the size of the message. Using
multiple algorithms allows one to minimize latency for small messages and
minimize bandwidth for long messages.</p>
<p>We will focus here on the implementation of broadcast in the <a class="reference external" href="http://www.mcs.anl.gov/mpi/mpich">MPICH2</a>
implementation. For short messages, MPICH2 uses a <a class="reference external" href="http://www.cs.auckland.ac.nz/~jmor159/PLDS210/trees.html">binomial tree</a> algorithm. In
this algorithm, the root process sends the data to one node in the first step,
and then in the subsequent, both the root and the other node can send the data
to other nodes. Thus, it takes a total of <img class="math" src="../_images/math/80b39ab3612493bf39ca358b1d1d8a7e48b93d9f.png" alt="\lceil \log_2 p \rceil"/> steps
to complete the communication. The time to complete the communication is</p>
<div class="math" id="equation-t-short">
<p><span class="eqno">(2)</span><img src="../_images/math/d877313c0f3481831adea680053c38e247995c6f.png" alt="t_{\text{short}} = \lceil \log_2 p \rceil \left ( \alpha + sN\beta \right )."/></p>
</div><p>This algorithm works well for short messages since the latency term scales
logarithmically with the number of nodes. However, for long messages, an
algorithm that has lower bandwidth has been proposed by <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.51.7772">Barnett</a> and implemented
in MPICH2. Rather than using a binomial tree, the broadcast is divided into a
scatter and an allgather. The time to complete the scatter is :math:` log_2 p
: alpha + frac{p-1}{p} Nbeta` using a binomial tree algorithm. The allgather
is performed using a ring algorithm that completes in <img class="math" src="../_images/math/2d3cfbb67f43244684dfd7f7d13b5a36e530d1fb.png" alt="p-1) \alpha +
\frac{p-1}{p} N\beta"/>. Thus, together the time to complete the broadcast is</p>
<div class="math" id="equation-t-broadcast">
<p><span class="eqno">(3)</span><img src="../_images/math/4e33c067e2123099c4eef1b9974563cb21e6c796.png" alt="t_{\text{long}} = \left ( \log_2 p + p - 1 \right ) \alpha + 2 \frac{p-1}{p}
sN\beta."/></p>
</div><p>The fission bank data will generally exceed the threshold for switching from
short to long messages (typically 8 kilobytes), and thus we will use the
equation for long messages. Adding equations <a href="#equation-t-send">(1)</a> and <a href="#equation-t-broadcast">(3)</a>,
the total cost of the series of sends and the broadcast is</p>
<div class="math" id="equation-t-old">
<p><span class="eqno">(4)</span><img src="../_images/math/67105d4b896a87e0d049f7fb875cd278e380e208.png" alt="t_{\text{old}} = \left ( \log_2 p + 2p - 1 \right ) \alpha + \frac{3p-2}{p}
sN\beta."/></p>
</div></div>
<div class="section" id="cost-of-nearest-neighbor-algorithm">
<h3>8.1.4. Cost of Nearest Neighbor Algorithm<a class="headerlink" href="#cost-of-nearest-neighbor-algorithm" title="Permalink to this headline">¶</a></h3>
<p>With the communication cost of the traditional fission bank algorithm
quantified, we now proceed to discuss the communicatin cost of the proposed
algorithm. Comparing the cost of communication of this algorithm with the
traditional algorithm is not trivial due to fact that the cost will be a
function of how many fission sites are sampled on each node. If each node
samples exactly <img class="math" src="../_images/math/c395cd3824fd2b81862d12d44397aa041de3156a.png" alt="N/p"/> sites, there will not be communication between nodes
at all. However, if any one node samples more or less than <img class="math" src="../_images/math/c395cd3824fd2b81862d12d44397aa041de3156a.png" alt="N/p"/> sites,
the deviation will result in communication between logically adjacent nodes. To
determine the expected deviation, one can analyze the process based on the
fundamentals of the Monte Carlo process.</p>
<p>The steady-state neutron transport equation for a multiplying medium can be
written in the form of an eigenvalue problem,</p>
<div class="math" id="equation-NTE">
<p><span class="eqno">(5)</span><img src="../_images/math/c84a08f8b1fd5eed92413a7efe3ba31a2bd520fa.png" alt="S(\mathbf{r})= \frac{1}{k} \int F(\mathbf{r}' \rightarrow
\mathbf{r})S(\mathbf{r}')\: d\mathbf{r},"/></p>
</div><p>where <img class="math" src="../_images/math/523765e6f77d2ff678e47c2a1ff0a59ace2e8e36.png" alt="\mathbf{r}"/> is the spatial coordinates of the neutron,
<img class="math" src="../_images/math/940b2b7c298023ee872920b7135b20eef2bfc7cc.png" alt="S(\mathbf{r})"/> is the source distribution defined as the expected number
of neutrons born from fission per unit phase-space volume at <img class="math" src="../_images/math/523765e6f77d2ff678e47c2a1ff0a59ace2e8e36.png" alt="\mathbf{r}"/>,
<img class="math" src="../_images/math/67b6776195b8e3228a1a5147377c45a7eb09741f.png" alt="F( \mathbf{r}' \rightarrow \mathbf{r})"/> is the expected number of
neutrons born from fission per unit phase space volume at <img class="math" src="../_images/math/523765e6f77d2ff678e47c2a1ff0a59ace2e8e36.png" alt="\mathbf{r}"/>
caused by a neutron at <img class="math" src="../_images/math/523765e6f77d2ff678e47c2a1ff0a59ace2e8e36.png" alt="\mathbf{r}"/>, and <img class="math" src="../_images/math/8c325612684d41304b9751c175df7bcc0f61f64f.png" alt="k"/> is the eigenvalue. The
fundamental eigenvalue of equation <a href="#equation-NTE">(5)</a> is known as <img class="math" src="../_images/math/56a4ca4edce4a1ea8da11edbc5adc54ed45163b2.png" alt="k_{eff}"/>, but
for simplicity we will simply refer to it as <img class="math" src="../_images/math/8c325612684d41304b9751c175df7bcc0f61f64f.png" alt="k"/>.</p>
<p>In a Monte Carlo criticality simulation, the power iteration method is applied
iteratively to obtain stochastic realizations of the source distribution and
estimates of the <img class="math" src="../_images/math/8c325612684d41304b9751c175df7bcc0f61f64f.png" alt="k"/>-eigenvalue. Let us define <img class="math" src="../_images/math/f05982465686929854ef764f71eecc94ad76845e.png" alt="\hat{S}^{(m)}"/> to be
the realization of the source distribution at cycle <img class="math" src="../_images/math/f5047d1e0cbb50ec208923a22cd517c55100fa7b.png" alt="m"/> and
<img class="math" src="../_images/math/85ef0389b5d2c7e90653ee534230f0dfbfc0ea97.png" alt="\hat{\epsilon}^{(m)}"/> be the noise arising from the stochastic nature of
the tracking process. We can write the stochastic realization in terms of the
fundamental source distribution and the noise component as (see <a class="reference external" href="http://dx.doi.org/10.1016/0306-4549(86)90095-2">Brissenden and
Garlick</a>):</p>
<div class="math" id="equation-source">
<p><span class="eqno">(6)</span><img src="../_images/math/5687a35a0550bac9c6f55ccf67f18330a34542b8.png" alt="\hat{S}^{(m)}(\mathbf{r})= N S(\mathbf{r}) + \sqrt{N}
\hat{\epsilon}^{(m)}(\mathbf{r}),"/></p>
</div><p>where <img class="math" src="../_images/math/fc97ef67268cd4e91bacdf12b8901d7036c9a056.png" alt="N"/> is the number of particle histories per cycle. Without loss of
generality, we shall drop the superscript notation indicating the cycle as it is
understood that the stochastic realization is at a particular cycle. The
expected value of the stochastic source distribution is simply</p>
<div class="math" id="equation-expected-value-source">
<p><span class="eqno">(7)</span><img src="../_images/math/3634a65aaf7d5325cf150034f0c759f41955be26.png" alt="E \left[ \hat{S}(\mathbf{r})\right] = N S (\mathbf{r})"/></p>
</div><p>since <img class="math" src="../_images/math/275dd79da679dacccf9f85d7ed5028e0c68735aa.png" alt="E \left[ \hat{\epsilon}(\mathbf{r})\right] = 0"/>. The noise in the
source distribution is due only to <img class="math" src="../_images/math/5cd8045db5708d33f68fbe6f31ea314e72c2224c.png" alt="\hat{\epsilon}(\mathbf{r})"/> and thus
the variance of the source distribution will be</p>
<div class="math" id="equation-var-source">
<p><span class="eqno">(8)</span><img src="../_images/math/148cb8c308703a08ca7c78e23fc0f8fecda1124e.png" alt="\text{Var} \left[ \hat{S}(\mathbf{r})\right] = N \text{Var} \left[
\hat{\epsilon}(\mathbf{r}) \right]."/></p>
</div><p>Lastly, the stochastic and true eigenvalues can be written as integrals over all
phase space of the stochastic and true source distributions, respectively, as</p>
<div class="math" id="equation-k-to-source">
<p><span class="eqno">(9)</span><img src="../_images/math/c9755e08705fc40ca865b2d9ab5a6daef2277c02.png" alt="\hat{k} = \frac{1}{N} \int \hat{S}(\mathbf{r}) \: d\mathbf{r} \quad
\text{and} \quad k = \int S(\mathbf{r}) \: d\mathbf{r},"/></p>
</div><p>noting that <img class="math" src="../_images/math/940b2b7c298023ee872920b7135b20eef2bfc7cc.png" alt="S(\mathbf{r})"/> is <img class="math" src="../_images/math/62d0effd6477f4244d585fc25f46a645378a4ceb.png" alt="O(1)"/>. One should note that the
expected value <img class="math" src="../_images/math/8c325612684d41304b9751c175df7bcc0f61f64f.png" alt="k"/> calculated by Monte Carlo power iteration (i.e. the
method of successive generations) will be biased from the true fundamental
eigenvalue of equation <a href="#equation-NTE">(5)</a> by <img class="math" src="../_images/math/d723af52337dd51e744bb956589f22d8ba52eeb0.png" alt="O(1/N)"/> (see <a class="reference external" href="http://dx.doi.org/10.1016/0306-4549(86)90095-2">Brissenden and
Garlick</a>), but we will assume henceforth that the number of particle histories
per cycle is sufficiently large to neglect this bias.</p>
<p>With this formalism, we now have a framework within which we can determine the
properties of the distribution of expected number of fission sites. The explicit
form of the source distribution can be written as</p>
<div class="math" id="equation-source-explicit">
<p><span class="eqno">(10)</span><img src="../_images/math/9fce3887457954b2e16b64c6fd5d9a65696c57eb.png" alt="\hat{S}(\mathbf{r}) = \sum_{i=1}^{M} w_i \delta( \mathbf{r} - \mathbf{r}_i )"/></p>
</div><p>where <img class="math" src="../_images/math/1357a3ea7b3d5d938c7f9daf28e6055832f9835e.png" alt="\mathbf{r}_i"/> is the spatial location of the <img class="math" src="../_images/math/34857b3ba74ce5cd8607f3ebd23e9015908ada71.png" alt="i"/>-th fission
site, <img class="math" src="../_images/math/535b2bfbb0a587e261a0d0af9b7b53e42629b14d.png" alt="w_i"/> is the statistical weight of the fission site at
<img class="math" src="../_images/math/1357a3ea7b3d5d938c7f9daf28e6055832f9835e.png" alt="\mathbf{r}_i"/>, and <img class="math" src="../_images/math/5d1e4485dc90c450e8c76826516c1b2ccb8fce16.png" alt="M"/> is the total number of fission sites. It is
clear that the total weight of the fission sites is simply the integral of the
source distribution. Integrating equation <a href="#equation-source">(6)</a> over all space, we obtain</p>
<div class="math" id="equation-source-integrated">
<p><span class="eqno">(11)</span><img src="../_images/math/25da7598817e81288a2761daf533422333e7a18c.png" alt="\int \hat{S}(\mathbf{r}) \: d\mathbf{r} = N \int S(\mathbf{r}) \:
d\mathbf{r} + \sqrt{N} \int \hat{\epsilon}(\mathbf{r}) \: d\mathbf{r} ."/></p>
</div><p>Substituting the expressions for the stochastic and true eigenvalues from
equation <a href="#equation-k-to-source">(9)</a>, we can relate the stochastic eigenvalue to the
integral of the noise component of the source distribution as</p>
<div class="math" id="equation-noise-integeral">
<p><span class="eqno">(12)</span><img src="../_images/math/786d76bfb9d32746bd82be11ad6ef1eb87c24730.png" alt="N\hat{k} = Nk + \sqrt{N} \int \hat{\epsilon}(\mathbf{r}) \: d\mathbf{r}."/></p>
</div><p>Since the expected value of <img class="math" src="../_images/math/9ada05558f1c9364ee3349687c0e7c6906b9adce.png" alt="\hat{\epsilon}"/> is zero, the expected value
of its integral will also be zero. We thus see that the variance of the integral
of the source distribution, i.e. the variance of the total weight of fission
sites produced, is directly proportional to the variance of the integral of the
noise component. Let us call this term <img class="math" src="../_images/math/741fb9098efcb98055f467f87630a5d0ca599b6b.png" alt="\sigma^2"/> for simplicity:</p>
<div class="math" id="equation-variance-sigma2">
<p><span class="eqno">(13)</span><img src="../_images/math/9c67861a3feee251052ccd0e8423cf6a7745831d.png" alt="\text{Var} \left[ \int \hat{S}(\mathbf{r}) \right ] = N \sigma^2."/></p>
</div><p>The actual value of <img class="math" src="../_images/math/741fb9098efcb98055f467f87630a5d0ca599b6b.png" alt="\sigma^2"/> will depend on the physical nature of the
problem, whether variance reduction techniques are employed, etc. For instance,
one could surmise that for a highly scattering problem, <img class="math" src="../_images/math/741fb9098efcb98055f467f87630a5d0ca599b6b.png" alt="\sigma^2"/> would
be smaller than for a highly absorbing problem since more collisions will lead
to a more precise estimate of the source distribution. Similarly, using implicit
capture should in theory reduce the value of <img class="math" src="../_images/math/741fb9098efcb98055f467f87630a5d0ca599b6b.png" alt="\sigma^2"/>.</p>
<p>Let us now consider the case where the <img class="math" src="../_images/math/fc97ef67268cd4e91bacdf12b8901d7036c9a056.png" alt="N"/> total histories are divided up
evenly across <img class="math" src="../_images/math/36f73fc1312ee0349b3f3a0f3bd9eb5504339011.png" alt="p"/> compute nodes. Since each node simulates <img class="math" src="../_images/math/c395cd3824fd2b81862d12d44397aa041de3156a.png" alt="N/p"/>
histories, we can write the source distribution as</p>
<div class="math" id="equation-source-node">
<p><span class="eqno">(14)</span><img src="../_images/math/0f0dec55139688164cef0fc40331f0d9265804f4.png" alt="\hat{S}_i(\mathbf{r})= \frac{N}{p} S(\mathbf{r}) + \sqrt{\frac{N}{p}}
\hat{\epsilon}_i(\mathbf{r}) \quad \text{for} \quad i = 1, \dots, p"/></p>
</div><p>Integrating over all space and simplifying, we can obtain an expression for the
eigenvalue on the <img class="math" src="../_images/math/34857b3ba74ce5cd8607f3ebd23e9015908ada71.png" alt="i"/>-th node:</p>
<div class="math" id="equation-k-i-hat">
<p><span class="eqno">(15)</span><img src="../_images/math/9446ee59de9bf61f6fddbaca53ac555824f94d98.png" alt="\hat{k}_i = k + \sqrt{\frac{p}{N}} \int \hat{\epsilon}_i(\mathbf{r}) \:
d\mathbf{r}."/></p>
</div><p>It is easy to show from this expression that the stochastic realization of the
global eigenvalue is merely the average of these local eigenvalues:</p>
<div class="math" id="equation-average-k-as-sum">
<p><span class="eqno">(16)</span><img src="../_images/math/4c27fcd8a73517681348759d61834aeeafa25c55.png" alt="\hat{k} = \frac{1}{p} \sum_{i=1}^p \hat{k}_i."/></p>
</div><p>As was mentioned earlier, at the end of each cycle one must sample <img class="math" src="../_images/math/fc97ef67268cd4e91bacdf12b8901d7036c9a056.png" alt="N"/>
sites from the <img class="math" src="../_images/math/5d1e4485dc90c450e8c76826516c1b2ccb8fce16.png" alt="M"/> sites that were created. Thus, the source for the next
cycle can be seen as the fission source from the current cycle divided by the
stochastic realization of the eigenvalue since it is clear from equation
<a href="#equation-k-to-source">(9)</a> that <img class="math" src="../_images/math/9b741a7e41f89312869b8aa5c07400719a191745.png" alt="\hat{k} = M/N"/>. Similarly, the number of sites
sampled on each compute node that will be used for the next cycle is</p>
<div class="math" id="equation-sites-per-node">
<p><span class="eqno">(17)</span><img src="../_images/math/b9075bdf821eb5e3ae3e404c2987ac541fa7bc4d.png" alt="M_i = \frac{1}{\hat{k}} \int \hat{S}_i(\mathbf{r}) \: d\mathbf{r} =
\frac{N}{p} \frac{\hat{k}_i}{\hat{k}}."/></p>
</div><p>While we know conceptually that each compute node will under normal
circumstances send two messages, many of these messages will overlap. Rather
than trying to determine the actual communication cost, we will instead attempt
to determine the maximum amount of data being communicated from one node to
another. At any given cycle, the number of fission sites that the <img class="math" src="../_images/math/8122aa89ea6e80784c6513d22787ad86e36ad0cc.png" alt="j"/>-th
compute node will send or receive (<img class="math" src="../_images/math/42c33d53819106d9c16057774cddf57154b5f560.png" alt="\Lambda_j"/>) is</p>
<div class="math" id="equation-Lambda">
<p><span class="eqno">(18)</span><img src="../_images/math/5ccde15a303c67b4d8ad0925c870ec03f09320ba.png" alt="\Lambda_j = \left | \sum_{i=1}^j M_i - \frac{jN}{p} \right |."/></p>
</div><p>Noting that <img class="math" src="../_images/math/c16f8d4a25c864ac30e71e5ac25eaea0a9b7408c.png" alt="jN/p"/> is the expected value of the summation, we can write
the expected value of <img class="math" src="../_images/math/42c33d53819106d9c16057774cddf57154b5f560.png" alt="\Lambda_j"/> as the mean absolute deviation of the
summation:</p>
<div class="math" id="equation-mean-dev-lambda">
<p><span class="eqno">(19)</span><img src="../_images/math/5919c9a67835f19eb6a5c8b3d47b9e035c052657.png" alt="E \left [ \Lambda_j \right ] = E \left [ \left | \sum_{i=1}^j M_i -
\frac{jN}{p} \right | \right ] = \text{MD} \left [ \sum_{i=1}^j M_i \right ]"/></p>
</div><p>where <img class="math" src="../_images/math/0415b987b655b923906c1471fc53dd6bbfe7a8c7.png" alt="\text{MD}"/> indicates the mean absolute deviation of a random
variable. The mean absolute deviation is an alternative measure of variability.</p>
<p>In order to ascertain any information about the mean deviation of <img class="math" src="../_images/math/50496bd204bbf34e209ecabe0729f4937fe69a1d.png" alt="M_i"/>,
we need to know the nature of its distribution. Thus far, we have said nothing
of the distributions of the random variables in question. The total number of
fission sites resulting from the tracking of <img class="math" src="../_images/math/fc97ef67268cd4e91bacdf12b8901d7036c9a056.png" alt="N"/> neutrons can be shown to
be normally distributed via the <a class="reference internal" href="tallies.html#central-limit-theorem"><em>Central Limit Theorem</em></a> (provided that
<img class="math" src="../_images/math/fc97ef67268cd4e91bacdf12b8901d7036c9a056.png" alt="N"/> is sufficiently large) since the fission sites resulting from each
neutron are &#8220;sampled&#8221; from independent, identically-distributed random
variables. Thus, <img class="math" src="../_images/math/c3b674e8ff5b70f0d1bd37d0d39760aa54502408.png" alt="\hat{k}"/> and <img class="math" src="../_images/math/0299a337f236d040bffecd93432b7a612a07871c.png" alt="\int \hat{S} (\mathbf{r}) \:
d\mathbf{r}"/> will be normally distributed as will the individual estimates of
these on each compute node.</p>
<p>Next, we need to know what the distribution of <img class="math" src="../_images/math/50496bd204bbf34e209ecabe0729f4937fe69a1d.png" alt="M_i"/> in equation
<a href="#equation-sites-per-node">(17)</a> is or, equivalently, how <img class="math" src="../_images/math/1fb3e3a1babedd0445a0e200afcb517b4fa942b6.png" alt="\hat{k}_i / \hat{k}"/> is
distributed. The distribution of a ratio of random variables is not easy to
calculate analytically, and it is not guaranteed that the ratio distribution is
normal if the numerator and denominator are normally distributed. For example,
if <img class="math" src="../_images/math/6a47ca0fe7cb276abc022af6ac88ddae1a9d6894.png" alt="X"/> is a standard normal distribution and <img class="math" src="../_images/math/ce58e4af225c93d08606c26554caaa5ae32edeba.png" alt="Y"/> is also standard
normal distribution, then the ratio <img class="math" src="../_images/math/5a1b0483983613ffc74e382713aac9c3058907aa.png" alt="X/Y"/> has the standard <a class="reference external" href="http://en.wikipedia.org/wiki/Cauchy_distribution">Cauchy
distribution</a>. The reader should be reminded that the Cauchy distribution has
no defined mean or variance. That being said, <a class="reference external" href="http://www.jstor.org/stable/10.2307/2342070">Geary</a> has shown that, for the
case of two normal distributions, if the denominator is unlikely to assume
values less than zero, then the ratio distribution is indeed approximately
normal. In our case, <img class="math" src="../_images/math/c3b674e8ff5b70f0d1bd37d0d39760aa54502408.png" alt="\hat{k}"/> absolutely cannot assume a value less than
zero, so we can be reasonably assured that the distribution of <img class="math" src="../_images/math/50496bd204bbf34e209ecabe0729f4937fe69a1d.png" alt="M_i"/> will
be normal.</p>
<p>For a normal distribution with mean <img class="math" src="../_images/math/2d8c833ed800824727cd7bd2fb9de1a12ad7e674.png" alt="\mu"/> and distribution function
<img class="math" src="../_images/math/c96dd6ec1dc4ad7520fbdc78fcdbec9edd068d0c.png" alt="f(x)"/>, it can be shown that</p>
<div class="math" id="equation-mean-dev-to-stdev">
<p><span class="eqno">(20)</span><img src="../_images/math/b6a1df04bfb7521e5518b69f4112f889be0328c7.png" alt="\int_{-\infty}^{\infty} f(x) \left | x - \mu \right | \: dx =
\sqrt{\frac{2}{\pi} \int_{-\infty}^{\infty} f(x) \left ( x - \mu \right )^2
\: dx}"/></p>
</div><p>and thus the mean absolute deviation is <img class="math" src="../_images/math/c9c55370f9cc4f17e6a6bd91c7651e8fbe2ff67d.png" alt="\sqrt{2/\pi}"/> times the standard
deviation. Therefore, to evaluate the mean absolute deviation of <img class="math" src="../_images/math/50496bd204bbf34e209ecabe0729f4937fe69a1d.png" alt="M_i"/>, we
need to first determine its variance. Substituting equation
<a href="#equation-average-k-as-sum">(16)</a> into equation <a href="#equation-sites-per-node">(17)</a>, we can rewrite
<img class="math" src="../_images/math/50496bd204bbf34e209ecabe0729f4937fe69a1d.png" alt="M_i"/> solely in terms of <img class="math" src="../_images/math/a4b36cc557943d410394cb213db426eee8a6b264.png" alt="\hat{k}_1, \dots, \hat{k}_p"/>:</p>
<div class="math" id="equation-M-i">
<p><span class="eqno">(21)</span><img src="../_images/math/3ee8a3b33422d3a20490f9bddf55898f62955679.png" alt="M_i = \frac{N \hat{k}_i}{\sum\limits_{j=1}^p \hat{k}_j}."/></p>
</div><p>Since we know the variance of <img class="math" src="../_images/math/860dfd42afb922c444933ea12f24ff0dbc967b46.png" alt="\hat{k}_i"/>, we can use the error
propagation law to determine the variance of <img class="math" src="../_images/math/50496bd204bbf34e209ecabe0729f4937fe69a1d.png" alt="M_i"/>:</p>
<div class="math" id="equation-M-variance">
<p><span class="eqno">(22)</span><img src="../_images/math/eb564c7cb814298b27a2499435491a59388d0bcf.png" alt="\text{Var} \left [ M_i \right ] = \sum_{j=1}^p \left ( \frac{\partial
M_i}{\partial \hat{k}_j} \right )^2 \text{Var} \left [ \hat{k}_j \right ] +
\sum\limits_{j \neq m} \sum\limits_{m=1}^p \left ( \frac{\partial
M_i}{\partial \hat{k}_j} \right ) \left ( \frac{\partial M_i}{\partial
\hat{k}_m} \right ) \text{Cov} \left [ \hat{k}_j, \hat{k}_m \right ]"/></p>
</div><p>where the partial derivatives are evaluated at <img class="math" src="../_images/math/5389cb73491a3a8a8a87e7d1e080824bd1914029.png" alt="\hat{k}_j = k"/>. Since
<img class="math" src="../_images/math/6f04d7c4f50443227e141ab0c211f4e2192184fe.png" alt="\hat{k}_j"/> and <img class="math" src="../_images/math/af8a46c0896f861c8d3626760cbf00689aae35a4.png" alt="\hat{k}_m"/> are independent if <img class="math" src="../_images/math/f2b2af70bb3cac0095ccc02ca2fce935291a6f3e.png" alt="j \neq m"/>,
their covariance is zero and thus the second term cancels out. Evaluating the
partial derivatives, we obtain</p>
<div class="math" id="equation-M-variance-2">
<p><span class="eqno">(23)</span><img src="../_images/math/d30ca4442445b11dccca4111e1e67b4e05c57d38.png" alt="\text{Var} \left [ M_i \right ] = \left ( \frac{N(p-1)}{kp^2} \right )^2
\frac{p\sigma^2}{N} + \sum_{j \neq i} \left ( \frac{-N}{kp^2} \right )^2
\frac{p\sigma^2}{N} = \frac{N(p-1)}{k^2p^2} \sigma^2."/></p>
</div><p>Through a similar analysis, one can show that the variance of
<img class="math" src="../_images/math/4e7deed6cb4e8257b7a19106c6451f0a7b67a7d8.png" alt="\sum_{i=1}^j M_i"/> is</p>
<div class="math" id="equation-sum-M-variance">
<p><span class="eqno">(24)</span><img src="../_images/math/b7255d303286a1deba6950b2b3c866725c0ff688.png" alt="\text{Var} \left [ \sum_{i=1}^j M_i \right ] = \frac{Nj(p-j)}{k^2p^2}
\sigma^2"/></p>
</div><p>Thus, the expected amount of communication on node <img class="math" src="../_images/math/8122aa89ea6e80784c6513d22787ad86e36ad0cc.png" alt="j"/>, i.e. the mean
absolute deviation of <img class="math" src="../_images/math/4e7deed6cb4e8257b7a19106c6451f0a7b67a7d8.png" alt="\sum_{i=1}^j M_i"/> is proportional to</p>
<div class="math" id="equation-communication-cost">
<p><span class="eqno">(25)</span><img src="../_images/math/1271a5033ea53711c7b12c29f2412d03e4a5d7d2.png" alt="E \left [ \Lambda_j \right ] = \sqrt{\frac{2Nj(p-j)\sigma^2}{\pi k^2p^2}}."/></p>
</div><p>This formula has all the properties that one would expect based on intuition:</p>
<blockquote>
<div><p>1. As the number of histories increases, the communication cost on each node
increases as well;</p>
<p>2. If <img class="math" src="../_images/math/9910fc8ed8a58c9874860c7482132fb871b8c502.png" alt="p=1"/>, i.e. if the problem is run on only one compute node, the
variance will be zero. This reflects the fact that exactly <img class="math" src="../_images/math/fc97ef67268cd4e91bacdf12b8901d7036c9a056.png" alt="N"/> sites
will be sampled if there is only one node.</p>
<p>3. For <img class="math" src="../_images/math/0ba7b5d931c9548c6455cf3b379182f5a823bf22.png" alt="j=p"/>, the variance will be zero. Again, this says that when
you sum the number of sites from each node, you will get exactly <img class="math" src="../_images/math/fc97ef67268cd4e91bacdf12b8901d7036c9a056.png" alt="N"/>
sites.</p>
</div></blockquote>
<p>We can determine the node that has the highest communication cost by
differentiating equation <a href="#equation-communication-cost">(25)</a> with respect to <img class="math" src="../_images/math/8122aa89ea6e80784c6513d22787ad86e36ad0cc.png" alt="j"/>,
setting it equal to zero, and solving for <img class="math" src="../_images/math/8122aa89ea6e80784c6513d22787ad86e36ad0cc.png" alt="j"/>. Doing so yields
<img class="math" src="../_images/math/9fa655cdf4434626d726389d22779b2cac321a13.png" alt="j_{\text{max}} = p/2"/>. Interestingly, substituting <img class="math" src="../_images/math/1b7335927e890ba327f0fa16ee9286992d7f5534.png" alt="j = p/2"/> in
equation <a href="#equation-communication-cost">(25)</a> shows us that the maximum communication cost
is actually independent of the number of nodes:</p>
<div class="math" id="equation-maximum-communication">
<p><span class="eqno">(26)</span><img src="../_images/math/03f2199fb277a74a8f5f62b84516a2977ffca261.png" alt="E \left [ \Lambda_{j_{\text{max}}} \right ] = \sqrt{ \frac{N\sigma^2}{2\pi
k^2}}."/></p>
</div></div>
</div>
<div class="section" id="references">
<h2>8.2. References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<table class="docutils citation" frame="void" id="troubetzkoy" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[Troubetzkoy]</a></td><td>E. Troubetzkoy, H. Steinberg, and M. Kalos, &#8220;Monte Carlo
Radiation Penetration Calculations on a Parallel Computer,&#8221;
<em>Trans. Am. Nucl. Soc.</em>, <strong>17</strong>, 260 (1973).</td></tr>
</tbody>
</table>
</div>
</div>


      </div>
      <div class="bottomnav">
      
        <p>
        «&#160;&#160;<a href="eigenvalue.html">7. Eigenvalue Calculations</a>
        &#160;&#160;::&#160;&#160;
        <a class="uplink" href="../index.html">Contents</a>
        &#160;&#160;::&#160;&#160;
        <a href="../usersguide/index.html">User&#8217;s Guide</a>&#160;&#160;»
        </p>

      </div>


    <div class="footer">
        &copy; Copyright 2011-2013, Massachusetts Institute of Technology.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-30411614-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

  </body>
</html>